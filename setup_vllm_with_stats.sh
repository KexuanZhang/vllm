#!/bin/bash

# Setup script for modified vLLM with stats logging
# This script installs the local vLLM in development mode

set -e  # Exit on error

echo "üöÄ Setting up modified vLLM with stats logging..."

# Get the directory of this script
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
VLLM_DIR="$SCRIPT_DIR"

echo "üìÅ Working in directory: $VLLM_DIR"

# Check if we're in the right directory
if [ ! -f "$VLLM_DIR/setup.py" ] || [ ! -f "$VLLM_DIR/pyproject.toml" ]; then
    echo "‚ùå Error: This doesn't appear to be a vLLM directory"
    echo "   Expected to find setup.py and pyproject.toml"
    exit 1
fi

# Check Python version
echo "üêç Checking Python version..."
python_version=$(python3 -c "import sys; print(f'{sys.version_info.major}.{sys.version_info.minor}')")
echo "   Python version: $python_version"

if [ "$python_version" != "3.8" ] && [ "$python_version" != "3.9" ] && [ "$python_version" != "3.10" ] && [ "$python_version" != "3.11" ]; then
    echo "‚ö†Ô∏è  Warning: vLLM typically requires Python 3.8-3.11, you have $python_version"
fi

# Check if CUDA is available
echo "üîß Checking CUDA availability..."
if command -v nvidia-smi &> /dev/null; then
    echo "   CUDA detected:"
    nvidia-smi --query-gpu=name,memory.total --format=csv,noheader,nounits | head -n 5
else
    echo "   ‚ö†Ô∏è  CUDA not detected - vLLM will run in CPU mode (very slow)"
fi

# Uninstall existing vLLM if present
echo "üßπ Checking for existing vLLM installation..."
if python3 -c "import vllm" 2>/dev/null; then
    echo "   Found existing vLLM installation. Uninstalling..."
    pip uninstall -y vllm || true
else
    echo "   No existing vLLM installation found."
fi

# Install dependencies
echo "üì¶ Installing build dependencies..."
pip install -U pip setuptools wheel
pip install ninja  # For faster compilation
pip install packaging  # Required for setup

# Add both the vLLM directory and the project directory to PYTHONPATH
echo "üîß Updating PYTHONPATH..."
export PYTHONPATH="$PYTHONPATH:/Users/zhang/Desktop/huawei/so1/vllm:/Users/zhang/Desktop/huawei/so1/semantic-operators/ggr-experiment-pipeline"
echo "   PYTHONPATH updated: $PYTHONPATH"

# Install in development mode
echo "üî® Installing modified vLLM in development mode..."
cd "$VLLM_DIR"

# Set environment variables for better compilation
export MAX_JOBS=4  # Limit parallel jobs to avoid memory issues
export NVCC_APPEND_FLAGS="-t 4"  # Limit NVCC threads

# Install in editable mode
pip install -e . --verbose

echo "‚úÖ Installation completed!"

# Verify installation
echo "üß™ Verifying installation..."
if python3 -c "import vllm; print(f'vLLM version: {vllm.__version__}'); print('‚úÖ Import successful!')" 2>/dev/null; then
    echo "‚úÖ vLLM installation verified successfully!"
else
    echo "‚ùå Installation verification failed!"
    exit 1
fi

# Test our custom wrapper
echo "üß™ Testing custom stats wrapper..."
if python3 -c "from vllm.offline_llm_with_stats import OfflineLLMWithStats; print('‚úÖ Custom wrapper import successful!')" 2>/dev/null; then
    echo "‚úÖ Custom wrapper verified successfully!"
else
    echo "‚ùå Custom wrapper verification failed!"
    exit 1
fi

echo ""
echo "üéâ Setup completed successfully!"
echo ""
echo "üìù Next steps:"
echo "   1. Test the installation: python3 test_vllm_stats_logging.py"
echo "   2. Use in your experiments with: from vllm.offline_llm_with_stats import OfflineLLMWithStats"
echo ""
echo "üí° Usage example:"
echo "   from vllm.offline_llm_with_stats import OfflineLLMWithStats"
echo "   from vllm import SamplingParams"
echo ""
echo "   # Using log_stats parameter (default way)"
echo "   llm = OfflineLLMWithStats(model='your/model/path', log_stats=True)"
echo "   outputs = llm.generate(['Hello world'], SamplingParams())"
echo ""
echo "   # Alternative: Using disable_log_stats parameter (for compatibility)"
echo "   # llm = OfflineLLMWithStats(model='your/model/path', disable_log_stats=True) # To disable stats"
echo "   # outputs = llm.generate(['Hello world'], SamplingParams())"
echo ""
